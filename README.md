# NeMo Fusion ğŸš€

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**Advanced Parallelism and Optimization Toolkit for NVIDIA NeMo Framework**

NeMo Fusion is a high-performance extension toolkit for [NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo) that provides advanced distributed training optimizations, intelligent parallelism strategies, and comprehensive profiling tools.

**Author**: [Abdullah Kazi](https://github.com/abdullah1kazi)

---

## ğŸ¯ Features

### ğŸ”§ Core Capabilities

- **Auto-Parallelism Optimizer**: Automatically determine optimal TP/PP/DP/CP configurations based on model size and hardware
- **Distributed Training Profiler**: Identify bottlenecks in GPU utilization, communication overhead, and memory usage
- **Memory-Efficient Attention**: Flash Attention and Ring Attention implementations compatible with NeMo
- **Mixed Precision Training**: FP8, FP16, BF16 training recipes optimized for H100/H200 GPUs
- **Multi-Modal Training Extensions**: Unified interface for text, image, and video modalities

### ğŸ“Š Performance Optimizations

- Hybrid parallelism strategies (TP + PP + DP + CP combinations)
- Communication overlap optimization
- Gradient accumulation optimizer
- Dynamic batch size scheduling
- Checkpoint compression utilities

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or higher
- NVIDIA GPU with CUDA support (for training)
- [UV](https://github.com/astral-sh/uv) package manager

### Installation

#### 1. Install UV (if not already installed)

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

#### 2. Clone the repository

```bash
git clone https://github.com/abdullah1kazi/nemo-fusion.git
cd nemo-fusion
```

#### 3. Install NeMo Fusion

**For users:**
```bash
uv sync
```

**For developers:**
```bash
uv sync --all-extras
```

**Quick install (without UV):**
```bash
pip install -e .
```

---

## ğŸ“– Usage Examples

### Example 1: Auto-Optimize LLaMA 70B Training

```python
from nemo_fusion.parallelism import AutoParallelOptimizer
from nemo_fusion.optimization import OptimizedTrainer

# Define model and hardware configuration
optimizer = AutoParallelOptimizer()

strategy = optimizer.optimize(
    num_params=70e9,        # 70B parameters
    num_layers=80,
    hidden_size=8192,
    num_gpus=8,
    gpu_memory_gb=80,
    batch_size=32,
    sequence_length=2048
)

print(f"Optimal config: TP={strategy.tensor_parallel}, PP={strategy.pipeline_parallel}")
print(f"Expected memory: {strategy.memory_per_gpu_gb:.1f} GB/GPU")
print(f"Expected throughput: {strategy.expected_throughput:.0f} tokens/sec")
```

### Example 2: Profile Training Bottlenecks

```python
from nemo_fusion.profiling import DistributedProfiler

profiler = DistributedProfiler()

with profiler.profile():
    # Your training loop
    for batch in dataloader:
        output = model(batch)
        loss.backward()
        optimizer.step()

# Get detailed analysis
report = profiler.analyze()
print(report.bottlenecks)
print(report.recommendations)
```

### Example 3: Multi-Modal Training

```python
from nemo_fusion.multimodal import UnifiedMultiModalTrainer

trainer = UnifiedMultiModalTrainer(
    text_encoder="llama-7b",
    vision_encoder="vit-large",
    fusion_strategy="cross_attention"
)

trainer.fit(dataloader)
```

---

## ğŸ› ï¸ Development

### Using UV for Development

```bash
# Install all dependencies including dev tools
uv sync --all-extras

# Run tests
uv run pytest tests/

# Run tests with coverage
make test-cov

# Format code
make format

# Run linters
make lint

# Build documentation
make docs
```

### Project Structure

```
nemo-fusion/
â”œâ”€â”€ nemo_fusion/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ parallelism/          # Auto-parallelism and hybrid strategies
â”‚   â”‚   â”œâ”€â”€ auto_parallel.py
â”‚   â”‚   â”œâ”€â”€ hybrid_parallel.py
â”‚   â”‚   â””â”€â”€ memory_efficient.py
â”‚   â”œâ”€â”€ profiling/            # Distributed training profilers
â”‚   â”‚   â”œâ”€â”€ bottleneck_analyzer.py
â”‚   â”‚   â”œâ”€â”€ gpu_profiler.py
â”‚   â”‚   â””â”€â”€ comm_profiler.py
â”‚   â”œâ”€â”€ optimization/         # Training optimizations
â”‚   â”‚   â”œâ”€â”€ mixed_precision.py
â”‚   â”‚   â”œâ”€â”€ gradient_optimization.py
â”‚   â”‚   â””â”€â”€ checkpoint_utils.py
â”‚   â””â”€â”€ multimodal/          # Multi-modal extensions
â”‚       â”œâ”€â”€ fusion_layers.py
â”‚       â”œâ”€â”€ data_pipeline.py
â”‚       â””â”€â”€ unified_trainer.py
â”œâ”€â”€ examples/                 # Usage examples
â”œâ”€â”€ benchmarks/              # Performance benchmarks
â”œâ”€â”€ tests/                   # Unit and integration tests
â”œâ”€â”€ pyproject.toml          # UV/Python project config
â”œâ”€â”€ Makefile                # Development commands
â”œâ”€â”€ QUICKSTART.md           # Quick start guide
â””â”€â”€ README.md
```

---

## ğŸ“š Documentation

- [Quick Start Guide](QUICKSTART.md) - Get started in 5 minutes
- [Examples](examples/) - Working code examples
- [Tests](tests/) - Unit and integration tests

---

## ğŸ¯ Alignment with NVIDIA NeMo

NeMo Fusion is designed to seamlessly integrate with NVIDIA NeMo Framework:

- âœ… Compatible with NeMo 2.0+ API
- âœ… Works with Megatron-Core parallelism strategies
- âœ… Supports NeMo's Tensor, Pipeline, Data, and Context Parallelism
- âœ… Integrates with PyTorch Lightning training loops
- âœ… Compatible with NeMo's checkpoint format

---

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Run `uv run pytest tests/` to verify
5. Submit a pull request

---

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo)
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
- [PyTorch Lightning](https://github.com/Lightning-AI/pytorch-lightning)

---

## ğŸ“§ Contact

**Author**: Abdullah Kazi
**GitHub**: [https://github.com/abdullah1kazi](https://github.com/abdullah1kazi)
**Project**: [https://github.com/abdullah1kazi/nemo-fusion](https://github.com/abdullah1kazi/nemo-fusion)

For questions and support, please open an issue on the [project repository](https://github.com/abdullah1kazi/nemo-fusion/issues).